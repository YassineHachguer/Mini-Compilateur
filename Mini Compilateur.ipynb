{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33babe8",
   "metadata": {},
   "source": [
    "<h1>Analyse lexicale</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856414d7",
   "metadata": {},
   "source": [
    "Voici une description simple pour votre cas :\n",
    "\n",
    "- **Constantes** : dans ce cas (nombres).\n",
    "- **Opérateurs** : `+`, `-`, `*`, `/`.\n",
    "- **Délimiteurs** : `(`, `)` (parenthèses ouvrantes et fermantes).\n",
    "- **Espaces blancs** : ignorés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f97fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# === Analyseur lexical ===\n",
    "class MathLexer:\n",
    "    rules = [\n",
    "        (\"NUMBER\", r\"\\d+\"),                 # Constante entière\n",
    "        (\"OP_ADD\", r\"\\+\"),                  # Opérateur d'addition\n",
    "        (\"OP_SUB\", r\"-\"),                   # Opérateur de soustraction\n",
    "        (\"OP_MUL\", r\"\\*\"),                  # Opérateur de multiplication\n",
    "        (\"OP_DIV\", r\"/\"),                   # Opérateur de division\n",
    "        (\"LPAREN\", r\"\\(\"),                  # Parenthèse ouvrante\n",
    "        (\"RPAREN\", r\"\\)\"),                  # Parenthèse fermante\n",
    "        (\"WHITESPACE\", r\"[ \\t]+\"),          # Espaces (à ignorer)\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        position = 0\n",
    "        while position < len(text):\n",
    "            match = None\n",
    "            for token_name, token_regex in self.rules:\n",
    "                regex = re.compile(token_regex)\n",
    "                match = regex.match(text, position)\n",
    "                if match:\n",
    "                    value = match.group(0)\n",
    "                    if token_name != \"WHITESPACE\":  # On ignore les espaces\n",
    "                        self.tokens.append((token_name, value))\n",
    "                    position += len(value)\n",
    "                    break\n",
    "            if not match:\n",
    "                raise ValueError(f\"Erreur lexicale : caractère inattendu '{text[position]}'\")\n",
    "        return self.tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d03811",
   "metadata": {},
   "source": [
    "<h1>Analyse syntaxique</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21715936",
   "metadata": {},
   "source": [
    "L'analyse syntaxique vérifie si l'expression respecte les règles de grammaire définies :\n",
    "\n",
    "**Expressions valides :**\n",
    "Somme ou différence entre termes : a + b, a - b.\n",
    "Produit ou division entre facteurs : a * b, a / b.\n",
    "Parenthèses imbriquées : (a + b) * c.\n",
    "\n",
    "**Termes** : Combinaison de constantes et d'opérateurs arithmétiques (+, -, *, /).\n",
    "**Facteurs** : Constantes ou expressions entourées par des parenthèses.\n",
    "**Erreurs détectées :**\n",
    "Parenthèses mal appariées : (a + b.\n",
    "Opérateurs sans opérandes : c +.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c466b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analyseur syntaxique ===\n",
    "class MathParser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.current_index = 0\n",
    "\n",
    "    def current_token(self):\n",
    "        if self.current_index < len(self.tokens):\n",
    "            return self.tokens[self.current_index]\n",
    "        return None\n",
    "\n",
    "    def advance(self):\n",
    "        self.current_index += 1\n",
    "\n",
    "    def match(self, token_type):\n",
    "        token = self.current_token()\n",
    "        if token and token[0] == token_type:\n",
    "            self.advance()\n",
    "            return token\n",
    "        return None\n",
    "\n",
    "    def parse(self):\n",
    "        result = self.parse_expression()\n",
    "        if self.current_token():\n",
    "            raise ValueError(\"Erreur syntaxique : tokens restants après l'analyse.\")\n",
    "        return result\n",
    "\n",
    "    def parse_expression(self):\n",
    "        # Règle : <EXP> ::= <TERM> ((+|-) <TERM>)*\n",
    "        result = self.parse_term()\n",
    "        while self.current_token() and self.current_token()[0] in {\"OP_ADD\", \"OP_SUB\"}:\n",
    "            operator = self.match(\"OP_ADD\") or self.match(\"OP_SUB\")\n",
    "            right = self.parse_term()\n",
    "            if operator[0] == \"OP_ADD\":\n",
    "                result += right\n",
    "            elif operator[0] == \"OP_SUB\":\n",
    "                result -= right\n",
    "        return result\n",
    "\n",
    "    def parse_term(self):\n",
    "        # Règle : <TERM> ::= <FACTOR> ((*|/) <FACTOR>)*\n",
    "        result = self.parse_factor()\n",
    "        while self.current_token() and self.current_token()[0] in {\"OP_MUL\", \"OP_DIV\"}:\n",
    "            operator = self.match(\"OP_MUL\") or self.match(\"OP_DIV\")\n",
    "            right = self.parse_factor()\n",
    "            if operator[0] == \"OP_MUL\":\n",
    "                result *= right\n",
    "            elif operator[0] == \"OP_DIV\":\n",
    "                if right == 0:\n",
    "                    raise ValueError(\"Erreur sémantique : division par zéro.\")\n",
    "                result /= right\n",
    "        return result\n",
    "\n",
    "    def parse_factor(self):\n",
    "        # Règle : <FACTOR> ::= NUMBER | (LPAREN EXP RPAREN)\n",
    "        if self.match(\"LPAREN\"):\n",
    "            result = self.parse_expression()\n",
    "            if not self.match(\"RPAREN\"):\n",
    "                raise ValueError(\"Erreur syntaxique : parenthèse fermante manquante.\")\n",
    "            return result\n",
    "        token = self.match(\"NUMBER\")\n",
    "        if token:\n",
    "            return int(token[1])\n",
    "        raise ValueError(\"Erreur syntaxique : facteur attendu.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930fe88",
   "metadata": {},
   "source": [
    "<h1>Analyse sémantique</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6020cb1",
   "metadata": {},
   "source": [
    "L'analyse sémantique vérifie la validité logique de l'expression :\n",
    "\n",
    "Division par zéro : Erreur si un opérateur / a une opérande droite égale à 0.\n",
    "Exemple : 10 / 0.\n",
    "Résultat attendu : Retourne un nombre valide après évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc43905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analyseur sémantique ===\n",
    "class MathSemanticAnalyzer:\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def analyze(self):\n",
    "        # Par exemple, vérifier si le résultat dépasse certaines limites\n",
    "        if self.result > 1e6:\n",
    "            raise ValueError(\"Erreur sémantique : le résultat dépasse la limite autorisée.\")\n",
    "        return self.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445306e",
   "metadata": {},
   "source": [
    "<h1>Application du code créé </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c16c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens lexicaux extraits : [('NUMBER', '5'), ('OP_ADD', '+'), ('LPAREN', '('), ('NUMBER', '6'), ('OP_MUL', '*'), ('NUMBER', '2'), ('RPAREN', ')'), ('OP_SUB', '-'), ('LPAREN', '('), ('NUMBER', '3'), ('OP_SUB', '-'), ('NUMBER', '8'), ('RPAREN', ')')]\n",
      "Résultat final après analyse sémantique : 22\n"
     ]
    }
   ],
   "source": [
    "# === Exemple d'utilisation ===\n",
    "input_expression = \"5 + (6 * 2) - (3-8) \"  # Notez qu'il y a une division par zéro pour tester l'analyse sémantique\n",
    "\n",
    "# Analyse lexicale\n",
    "lexer = MathLexer()\n",
    "tokens = lexer.tokenize(input_expression)\n",
    "print(\"Tokens lexicaux extraits :\", tokens)\n",
    "\n",
    "# Analyse syntaxique\n",
    "parser = MathParser(tokens)\n",
    "try:\n",
    "    result = parser.parse()\n",
    "\n",
    "    # Analyse sémantique\n",
    "    semantic_analyzer = MathSemanticAnalyzer(result)\n",
    "    final_result = semantic_analyzer.analyze()\n",
    "    print(\"Résultat final après analyse sémantique :\", final_result)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Erreur : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
